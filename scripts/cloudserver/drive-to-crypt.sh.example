#!/bin/bash
# This script is to convert my unencrypted media on GDrive to Encrypted. It would run every day (cron job), encrypting the files in 300 to 500 GB batches.

#status=`ps -efww | grep -w "[d]rive-to-crypt.sh" | awk -vpid=$$ '$2 != pid { print $2 }'`
#if [ ! -z "$status" ]; then
#    echo "[`date`] : drive-to-crypt.sh : Process is already running"
#    exit 1;
#fi

# Variables to verify match your installation
DOCKERFOLDER=/home/USER/docker
RCLONE_USER=USER

# Rclone Config file
RCLONE_CONFIG=$DOCKERFOLDER/appdata/rclone/rclone.conf
export RCLONE_CONFIG

# User account that Rclone is running as
RCLONE_USER_AGENT=$RCLONE_USER
export RCLONE_USER_AGENT

# Local Mount - Ensure this is set to your local disk only.
#LOCAL=/media/shb-drive/media
EXCLUDES=$DOCKERFOLDER/scripts/cloudserver/upload-excludes.log
LOGFILE=$DOCKERFOLDER/logs/cloudserver/drive-to-crypt.log

# Name of your Rclone Remote, likely cloud or cloudcrypt
REMOTE_CRYPT=shb-crypt
REMOTE_DRIVE=shb-drive

# This will move pretty much everying (older than 1s)
MOVEAGE=1s

# Checks
# Running Check
if [[ $(pidof -x "$(basename "$0")" -o %PPID) ]]; then exit; fi

# Exclude File Check
if [[ ! -f $EXCLUDES ]] ; then
    echo 'Warning: No excludes.log file found at $EXCLUDES'
    exit 1
fi

# Local Disk Check
#if /bin/findmnt $LOCAL -o FSTYPE -n | grep fuse; then
#        echo "Warning: $LOCAL is not a local disk!"
#        exit 1
#fi

# Rclone Move Command. 
# The bwlimit will will average around 6 MB/s, which equates to about 506.25 GB upload per day. 
# Or use max-transfer of 500 GB, so there is some bandwidth left for other purposes.
# Google Drive has a maximum of 750 GB upload per day per user account
/usr/bin/rclone move $REMOTE_DRIVE:/media $REMOTE_CRYPT:/media --log-file $LOGFILE -v --max-transfer=300G --delete-empty-src-dirs --cutoff-mode=soft --exclude-from $EXCLUDES --fast-list --min-age $MOVEAGE

exit 0
